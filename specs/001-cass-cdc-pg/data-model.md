# Data Model: Cassandra-to-PostgreSQL CDC Pipeline

**Date**: 2025-11-20
**Feature**: 001-cass-cdc-pg
**Purpose**: Define core data entities, schemas, and relationships for the CDC pipeline

## Overview

This document defines the data models for the CDC pipeline components. All models are implementation-agnostic at this level, with concrete Python Pydantic implementations in `/contracts/data-models/`.

---

## 1. Change Event

**Description**: Represents a single data change captured from Cassandra by Debezium.

### Fields

| Field | Type | Required | Description |
|-------|------|----------|-------------|
| `event_id` | UUID | Yes | Unique identifier for this event (generated by pipeline) |
| `source_table` | String | Yes | Fully qualified table name (`keyspace.table`) |
| `operation` | Enum | Yes | Operation type: `INSERT`, `UPDATE`, `DELETE` |
| `primary_key` | Map<String, Any> | Yes | Cassandra primary key columns and values |
| `before_data` | Map<String, Any> | No | Column values before change (NULL for INSERT) |
| `after_data` | Map<String, Any> | No | Column values after change (NULL for DELETE) |
| `timestamp` | Timestamp | Yes | Cassandra write timestamp (microseconds since epoch) |
| `cassandra_node` | String | Yes | Cassandra node hostname that wrote the change |
| `kafka_metadata` | KafkaMetadata | Yes | Kafka topic, partition, offset information |
| `schema_version` | Integer | Yes | Schema version number from schema registry |
| `trace_id` | String | Yes | Distributed tracing correlation ID |

### Validation Rules

- `operation` must be one of: `INSERT`, `UPDATE`, `DELETE`
- For `INSERT`: `before_data` must be NULL, `after_data` must be non-empty
- For `UPDATE`: Both `before_data` and `after_data` must be non-empty
- For `DELETE`: `before_data` must be non-empty, `after_data` must be NULL
- `timestamp` must be within reasonable range (not future, not >7 days past)
- `primary_key` must contain all partition key + clustering key columns

### State Transitions

```text
Cassandra Write → Debezium Capture → Kafka Publish → Worker Consume → Validation → Transformation → PostgreSQL Write → Checkpoint Commit
```

### Example (JSON)

```json
{
  "event_id": "a1b2c3d4-5678-90ab-cdef-1234567890ab",
  "source_table": "prod_keyspace.users",
  "operation": "UPDATE",
  "primary_key": {
    "user_id": "e5f6g7h8-9012-34ij-klmn-5678901234op"
  },
  "before_data": {
    "email": "old@example.com",
    "name": "John Doe",
    "age": 29
  },
  "after_data": {
    "email": "new@example.com",
    "name": "John Doe",
    "age": 30
  },
  "timestamp": 1700500000123456,
  "cassandra_node": "cass-node-01.example.com",
  "kafka_metadata": {
    "topic": "cassandra.prod_keyspace.users",
    "partition": 3,
    "offset": 123456,
    "timestamp": 1700500000500
  },
  "schema_version": 2,
  "trace_id": "trace-abc123xyz"
}
```

---

## 2. Replication Checkpoint

**Description**: Tracks pipeline progress for each Cassandra table partition to enable resumption after restart.

### Fields

| Field | Type | Required | Description |
|-------|------|----------|-------------|
| `checkpoint_id` | String | Yes | Composite key: `{table}:{partition}` |
| `table_name` | String | Yes | Fully qualified table name (`keyspace.table`) |
| `partition_id` | Integer | Yes | Kafka partition number |
| `kafka_offset` | Integer | Yes | Last successfully committed Kafka offset |
| `timestamp` | Timestamp | Yes | Timestamp of last committed event |
| `worker_id` | String | Yes | Worker instance that owns this partition |
| `record_count` | Integer | Yes | Number of records processed since last checkpoint |
| `last_updated` | Timestamp | Yes | System timestamp when checkpoint was updated |
| `schema_version` | Integer | Yes | Schema version at time of checkpoint |

### Validation Rules

- `kafka_offset` must be >= 0
- `timestamp` must be <= current time
- `worker_id` must match active worker (verified via Redis heartbeat)
- `last_updated` must be recent (within 60 seconds for active workers)
- `record_count` resets to 0 after each checkpoint commit

### Storage

- Stored in Redis: Key = `checkpoint:{table_name}:{partition_id}`
- TTL: None (persist indefinitely, delete manually if table dropped)
- Backup: Redis RDB snapshots + AOF log

### Example (JSON)

```json
{
  "checkpoint_id": "prod_keyspace.users:3",
  "table_name": "prod_keyspace.users",
  "partition_id": 3,
  "kafka_offset": 123456,
  "timestamp": 1700500000123456,
  "worker_id": "cdc-worker-02",
  "record_count": 1000,
  "last_updated": "2025-11-20T10:15:30.123Z",
  "schema_version": 2
}
```

---

## 3. Schema Definition

**Description**: Cached metadata about Cassandra table structure to enable validation and transformation.

### Fields

| Field | Type | Required | Description |
|-------|------|----------|-------------|
| `schema_id` | String | Yes | Composite key: `{keyspace}:{table}` |
| `keyspace` | String | Yes | Cassandra keyspace name |
| `table_name` | String | Yes | Cassandra table name |
| `version` | Integer | Yes | Schema version number (incremented on DDL changes) |
| `columns` | List<ColumnDefinition> | Yes | List of column definitions |
| `primary_key` | List<String> | Yes | Ordered list of primary key column names |
| `clustering_columns` | List<String> | Yes | Ordered list of clustering column names (subset of primary_key) |
| `indexes` | List<IndexDefinition> | No | Secondary indexes (if any) |
| `created_at` | Timestamp | Yes | When schema was first cached |
| `updated_at` | Timestamp | Yes | When schema was last updated |
| `cassandra_version` | String | Yes | Cassandra version (e.g., "4.1.3") |

### ColumnDefinition

| Field | Type | Required | Description |
|-------|------|----------|-------------|
| `name` | String | Yes | Column name |
| `cassandra_type` | String | Yes | Cassandra CQL type (e.g., `text`, `int`, `uuid`, `list<int>`) |
| `postgres_type` | String | Yes | Mapped PostgreSQL type (e.g., `text`, `integer`, `uuid`, `integer[]`) |
| `nullable` | Boolean | Yes | Whether column can be NULL |
| `default_value` | Any | No | Default value (if defined) |

### IndexDefinition

| Field | Type | Required | Description |
|-------|------|----------|-------------|
| `name` | String | Yes | Index name |
| `column` | String | Yes | Indexed column name |
| `index_type` | String | Yes | Index type (e.g., `SECONDARY`, `COMPOSITE`) |

### Storage

- Stored in Redis: Key = `schema:{keyspace}:{table}`
- TTL: 3600 seconds (1 hour), refreshed on access
- Backup: Persisted in PostgreSQL metadata table for disaster recovery

### Validation Rules

- `version` must increment monotonically
- `primary_key` must be non-empty and ordered (partition keys first, then clustering keys)
- All `primary_key` columns must exist in `columns` list
- Column names must be unique within table

### Example (JSON)

```json
{
  "schema_id": "prod_keyspace:users",
  "keyspace": "prod_keyspace",
  "table_name": "users",
  "version": 2,
  "columns": [
    {
      "name": "user_id",
      "cassandra_type": "uuid",
      "postgres_type": "uuid",
      "nullable": false
    },
    {
      "name": "email",
      "cassandra_type": "text",
      "postgres_type": "text",
      "nullable": false
    },
    {
      "name": "name",
      "cassandra_type": "text",
      "postgres_type": "text",
      "nullable": true
    },
    {
      "name": "age",
      "cassandra_type": "int",
      "postgres_type": "integer",
      "nullable": true
    },
    {
      "name": "tags",
      "cassandra_type": "list<text>",
      "postgres_type": "text[]",
      "nullable": true
    },
    {
      "name": "metadata",
      "cassandra_type": "map<text, text>",
      "postgres_type": "jsonb",
      "nullable": true
    },
    {
      "name": "created_at",
      "cassandra_type": "timestamp",
      "postgres_type": "timestamptz",
      "nullable": false
    }
  ],
  "primary_key": ["user_id"],
  "clustering_columns": [],
  "indexes": [
    {
      "name": "users_email_idx",
      "column": "email",
      "index_type": "SECONDARY"
    }
  ],
  "created_at": "2025-11-20T09:00:00Z",
  "updated_at": "2025-11-20T10:00:00Z",
  "cassandra_version": "4.1.3"
}
```

---

## 4. Error Record

**Description**: Represents a failed replication attempt requiring manual intervention or retry.

### Fields

| Field | Type | Required | Description |
|-------|------|----------|-------------|
| `error_id` | UUID | Yes | Unique identifier for this error |
| `original_event` | ChangeEvent | Yes | The event that failed processing |
| `error_type` | Enum | Yes | Error category: `VALIDATION`, `TRANSFORMATION`, `CONSTRAINT`, `NETWORK`, `UNKNOWN` |
| `error_message` | String | Yes | Human-readable error description |
| `error_stacktrace` | String | No | Full exception stacktrace (for debugging) |
| `attempt_count` | Integer | Yes | Number of retry attempts (max 10) |
| `first_failure_timestamp` | Timestamp | Yes | When error first occurred |
| `last_failure_timestamp` | Timestamp | Yes | Most recent retry attempt |
| `source_kafka_topic` | String | Yes | Kafka topic event came from |
| `source_kafka_partition` | Integer | Yes | Kafka partition |
| `source_kafka_offset` | Integer | Yes | Kafka offset |
| `worker_id` | String | Yes | Worker that encountered error |
| `trace_id` | String | Yes | Distributed tracing correlation ID |
| `status` | Enum | Yes | Error status: `PENDING_RETRY`, `FAILED`, `RESOLVED`, `ARCHIVED` |

### Error Types

- **VALIDATION**: Schema mismatch, missing required field, type mismatch
- **TRANSFORMATION**: Data type conversion failure (e.g., invalid timestamp format)
- **CONSTRAINT**: PostgreSQL constraint violation (unique, foreign key, check)
- **NETWORK**: Transient network error (timeout, connection refused)
- **UNKNOWN**: Unexpected exception

### Validation Rules

- `attempt_count` must be 1-10 (after 10 attempts, move to `FAILED` status)
- `last_failure_timestamp` must be >= `first_failure_timestamp`
- `status` transitions: `PENDING_RETRY` → `FAILED` or `PENDING_RETRY` → `RESOLVED`

### Storage

- Published to Kafka DLQ topic: `cassandra.cdc.dlq`
- Retention: 30 days (configurable)
- Also stored in PostgreSQL `error_log` table for querying/analytics

### Example (JSON)

```json
{
  "error_id": "error-123e4567-e89b-12d3-a456-426614174000",
  "original_event": { /* Full ChangeEvent */ },
  "error_type": "VALIDATION",
  "error_message": "Column 'age' expected integer, got string 'thirty'",
  "error_stacktrace": "Traceback (most recent call last):\\n  File ...",
  "attempt_count": 3,
  "first_failure_timestamp": "2025-11-20T10:00:00Z",
  "last_failure_timestamp": "2025-11-20T10:05:00Z",
  "source_kafka_topic": "cassandra.prod_keyspace.users",
  "source_kafka_partition": 3,
  "source_kafka_offset": 123456,
  "worker_id": "cdc-worker-02",
  "trace_id": "trace-abc123xyz",
  "status": "PENDING_RETRY"
}
```

---

## 5. Pipeline Worker

**Description**: Metadata about a running worker instance for coordination and monitoring.

### Fields

| Field | Type | Required | Description |
|-------|------|----------|-------------|
| `worker_id` | String | Yes | Unique worker identifier (hostname + PID) |
| `hostname` | String | Yes | Container/VM hostname |
| `ip_address` | String | Yes | Worker IP address |
| `start_time` | Timestamp | Yes | When worker started |
| `last_heartbeat` | Timestamp | Yes | Last heartbeat timestamp (updated every 10s) |
| `status` | Enum | Yes | Worker status: `STARTING`, `RUNNING`, `DRAINING`, `STOPPED` |
| `assigned_tables` | List<String> | Yes | Tables/partitions assigned to this worker |
| `version` | String | Yes | Pipeline version (semantic version) |
| `configuration` | Map<String, Any> | No | Worker configuration snapshot |
| `metrics_endpoint` | String | Yes | Prometheus metrics URL (e.g., `http://worker:8000/metrics`) |

### Validation Rules

- `last_heartbeat` must be within 30 seconds of current time for worker to be considered alive
- `status` transitions: `STARTING` → `RUNNING` → `DRAINING` → `STOPPED`
- Workers with `last_heartbeat` older than 30s are considered dead and reassigned

### Storage

- Stored in Redis: Key = `worker:{worker_id}`
- TTL: 60 seconds (auto-expires if worker crashes without graceful shutdown)
- Heartbeat updates TTL on each write

### Example (JSON)

```json
{
  "worker_id": "cdc-worker-02-12345",
  "hostname": "cdc-worker-02",
  "ip_address": "10.0.1.42",
  "start_time": "2025-11-20T09:00:00Z",
  "last_heartbeat": "2025-11-20T10:15:25Z",
  "status": "RUNNING",
  "assigned_tables": [
    "prod_keyspace.users:partition-3",
    "prod_keyspace.orders:partition-5"
  ],
  "version": "1.0.0",
  "configuration": {
    "batch_size": 1000,
    "checkpoint_interval_seconds": 10
  },
  "metrics_endpoint": "http://10.0.1.42:8000/metrics"
}
```

---

## 6. Metadata Cache Entry

**Description**: Generic key-value entry for storing pipeline configuration and locks in Redis.

### Fields

| Field | Type | Required | Description |
|-------|------|----------|-------------|
| `key` | String | Yes | Redis key (namespace-prefixed) |
| `value` | String | Yes | Serialized value (JSON for structured data) |
| `ttl` | Integer | No | Time-to-live in seconds (NULL for no expiration) |
| `created_at` | Timestamp | Yes | When entry was created |
| `updated_at` | Timestamp | Yes | When entry was last updated |

### Key Patterns

- **Configuration**: `config:{environment}:{key}` → JSON value
- **Distributed Locks**: `lock:{table}` → `{worker_id, acquired_at}`
- **Schema Cache**: `schema:{keyspace}:{table}` → JSON SchemaDefinition
- **Checkpoint**: `checkpoint:{table}:{partition}` → JSON ReplicationCheckpoint

### Example (JSON)

```json
{
  "key": "config:prod:batch_size",
  "value": "1000",
  "ttl": null,
  "created_at": "2025-11-20T09:00:00Z",
  "updated_at": "2025-11-20T09:00:00Z"
}
```

---

## 7. Dead Letter Record

**Description**: Change event that exceeded retry threshold, preserved for manual review.

### Fields

| Field | Type | Required | Description |
|-------|------|----------|-------------|
| `dlq_id` | UUID | Yes | Unique identifier for DLQ entry |
| `error_record` | ErrorRecord | Yes | Full error record with all retry attempts |
| `archived_at` | Timestamp | Yes | When event was moved to DLQ |
| `resolution_status` | Enum | Yes | `PENDING`, `RESOLVED`, `PERMANENT_FAILURE` |
| `resolution_notes` | String | No | Operator notes (e.g., "Corrected age field, resubmitted") |
| `resolved_by` | String | No | Operator username who resolved issue |
| `resolved_at` | Timestamp | No | When issue was resolved |

### Validation Rules

- Event must have `attempt_count >= 10` to be archived to DLQ
- `resolution_status` starts as `PENDING`, transitions to `RESOLVED` or `PERMANENT_FAILURE`

### Storage

- Kafka topic: `cassandra.cdc.dlq`
- Also mirrored to PostgreSQL `dlq_archive` table for long-term storage and querying

### Example (JSON)

```json
{
  "dlq_id": "dlq-98765432-abcd-ef01-2345-67890abcdef0",
  "error_record": { /* Full ErrorRecord */ },
  "archived_at": "2025-11-20T10:10:00Z",
  "resolution_status": "PENDING",
  "resolution_notes": null,
  "resolved_by": null,
  "resolved_at": null
}
```

---

## 8. Staleness Audit Entry

**Description**: Record of rejected stale events for audit and analysis.

### Fields

| Field | Type | Required | Description |
|-------|------|----------|-------------|
| `audit_id` | UUID | Yes | Unique identifier for audit entry |
| `event_id` | UUID | Yes | ID of rejected stale event |
| `source_table` | String | Yes | Table name |
| `primary_key` | Map<String, Any> | Yes | Primary key of affected record |
| `event_timestamp` | Timestamp | Yes | Timestamp from stale event |
| `current_record_timestamp` | Timestamp | Yes | Timestamp of current PostgreSQL record |
| `staleness_threshold` | Integer | Yes | Configured staleness threshold (seconds) |
| `rejection_reason` | String | Yes | Why event was rejected |
| `rejected_at` | Timestamp | Yes | When rejection occurred |
| `worker_id` | String | Yes | Worker that rejected event |
| `trace_id` | String | Yes | Distributed tracing correlation ID |

### Validation Rules

- `event_timestamp` must be < `current_record_timestamp` (that's why it's stale)
- `staleness_threshold` typically 604800 seconds (7 days)

### Storage

- Kafka topic: `cassandra.cdc.staleness_audit`
- Retention: 90 days
- Queryable via PostgreSQL `staleness_audit` table

### Example (JSON)

```json
{
  "audit_id": "audit-11111111-2222-3333-4444-555555555555",
  "event_id": "event-aaaaaaaa-bbbb-cccc-dddd-eeeeeeeeeeee",
  "source_table": "prod_keyspace.users",
  "primary_key": {
    "user_id": "user-12345"
  },
  "event_timestamp": "2025-11-13T10:00:00Z",
  "current_record_timestamp": "2025-11-20T10:00:00Z",
  "staleness_threshold": 604800,
  "rejection_reason": "Event timestamp 7+ days older than current record",
  "rejected_at": "2025-11-20T10:15:30Z",
  "worker_id": "cdc-worker-02",
  "trace_id": "trace-xyz789"
}
```

---

## 9. Validation Error

**Description**: Record that failed schema validation with detailed error information.

### Fields

| Field | Type | Required | Description |
|-------|------|----------|-------------|
| `validation_error_id` | UUID | Yes | Unique identifier |
| `event_id` | UUID | Yes | ID of event that failed validation |
| `source_table` | String | Yes | Table name |
| `validation_failures` | List<ValidationFailure> | Yes | List of specific validation errors |
| `original_data` | Map<String, Any> | Yes | Original event data (preserved for correction) |
| `expected_schema` | SchemaDefinition | Yes | Schema against which validation failed |
| `failed_at` | Timestamp | Yes | When validation failed |
| `worker_id` | String | Yes | Worker that detected failure |
| `trace_id` | String | Yes | Distributed tracing correlation ID |
| `resolution_status` | Enum | Yes | `PENDING`, `CORRECTED`, `IGNORED` |

### ValidationFailure

| Field | Type | Required | Description |
|-------|------|----------|-------------|
| `field_name` | String | Yes | Column name that failed validation |
| `error_type` | String | Yes | Error type: `MISSING_REQUIRED`, `TYPE_MISMATCH`, `CONSTRAINT_VIOLATION` |
| `expected_value` | String | No | Expected value or type |
| `actual_value` | String | Yes | Actual value received |
| `error_message` | String | Yes | Human-readable error description |

### Storage

- Kafka topic: `cassandra.cdc.validation_errors`
- Also stored in PostgreSQL `validation_errors` table

### Example (JSON)

```json
{
  "validation_error_id": "val-error-12345",
  "event_id": "event-67890",
  "source_table": "prod_keyspace.users",
  "validation_failures": [
    {
      "field_name": "age",
      "error_type": "TYPE_MISMATCH",
      "expected_value": "integer",
      "actual_value": "thirty",
      "error_message": "Expected integer type, got string 'thirty'"
    },
    {
      "field_name": "email",
      "error_type": "CONSTRAINT_VIOLATION",
      "expected_value": "valid email format",
      "actual_value": "not-an-email",
      "error_message": "Invalid email format"
    }
  ],
  "original_data": {
    "user_id": "user-123",
    "email": "not-an-email",
    "age": "thirty"
  },
  "expected_schema": { /* SchemaDefinition */ },
  "failed_at": "2025-11-20T10:20:00Z",
  "worker_id": "cdc-worker-03",
  "trace_id": "trace-validation-456",
  "resolution_status": "PENDING"
}
```

---

## 10. Metric Data Point

**Description**: Time-series measurement of pipeline performance for monitoring.

### Fields

| Field | Type | Required | Description |
|-------|------|----------|-------------|
| `metric_name` | String | Yes | Metric identifier (e.g., `cdc_events_processed_total`) |
| `metric_type` | Enum | Yes | Type: `COUNTER`, `GAUGE`, `HISTOGRAM` |
| `value` | Float | Yes | Metric value |
| `labels` | Map<String, String> | Yes | Metric labels for filtering (e.g., `{table: "users", worker_id: "worker-02"}`) |
| `timestamp` | Timestamp | Yes | When metric was recorded |

### Key Metrics (from spec)

- `cdc_events_processed_total` (Counter): Total events processed
- `cdc_replication_latency_seconds` (Histogram): End-to-end latency
- `cdc_replication_lag_seconds` (Gauge): Lag between Kafka head and committed offset
- `cdc_error_total` (Counter): Errors by type
- `cdc_dlq_size` (Gauge): DLQ topic size

### Storage

- Exposed via Prometheus endpoint: `/metrics`
- Scraped by Prometheus server every 15 seconds
- Stored in Prometheus TSDB with 30-day retention

### Example (Prometheus format)

```text
# HELP cdc_events_processed_total Total CDC events processed
# TYPE cdc_events_processed_total counter
cdc_events_processed_total{table="prod_keyspace.users",worker_id="cdc-worker-02"} 123456.0

# HELP cdc_replication_latency_seconds End-to-end replication latency
# TYPE cdc_replication_latency_seconds histogram
cdc_replication_latency_seconds_bucket{table="prod_keyspace.users",le="0.5"} 1000.0
cdc_replication_latency_seconds_bucket{table="prod_keyspace.users",le="1.0"} 5000.0
cdc_replication_latency_seconds_bucket{table="prod_keyspace.users",le="+Inf"} 10000.0
cdc_replication_latency_seconds_sum{table="prod_keyspace.users"} 15000.0
cdc_replication_latency_seconds_count{table="prod_keyspace.users"} 10000.0
```

---

## 11. Kafka Metadata (Nested)

**Description**: Kafka-specific metadata attached to each ChangeEvent.

### Fields

| Field | Type | Required | Description |
|-------|------|----------|-------------|
| `topic` | String | Yes | Kafka topic name |
| `partition` | Integer | Yes | Kafka partition number |
| `offset` | Integer | Yes | Kafka message offset |
| `timestamp` | Timestamp | Yes | Kafka message timestamp (when published) |
| `key` | String | No | Kafka message key (typically primary key hash) |
| `headers` | Map<String, String> | No | Kafka message headers (e.g., `trace_id`, `schema_version`) |

### Example (JSON)

```json
{
  "topic": "cassandra.prod_keyspace.users",
  "partition": 3,
  "offset": 123456,
  "timestamp": 1700500000500,
  "key": "user-12345",
  "headers": {
    "trace_id": "trace-abc123",
    "schema_version": "2"
  }
}
```

---

## Relationships & Data Flow

```text
┌─────────────┐
│  Cassandra  │
│   (Source)  │
└──────┬──────┘
       │ CDC Commitlog
       ▼
┌─────────────┐
│  Debezium   │
│  Connector  │
└──────┬──────┘
       │ Publish
       ▼
┌─────────────┐      ┌──────────────┐
│    Kafka    │◄────►│ Schema       │
│   Topics    │      │ Registry     │
└──────┬──────┘      └──────────────┘
       │ Consume
       ▼
┌─────────────┐      ┌──────────────┐
│ CDC Worker  │◄────►│ Redis Cache  │
│  (Python)   │      │ (Metadata)   │
└──────┬──────┘      └──────────────┘
       │ Validate & Transform
       ├──────────┬──────────┬─────────┐
       │          │          │         │
       ▼          ▼          ▼         ▼
┌──────────┐ ┌────────┐ ┌──────┐ ┌────────┐
│PostgreSQL│ │  DLQ   │ │Audit │ │Metrics │
│ (Target) │ │ Topic  │ │Topic │ │Endpoint│
└──────────┘ └────────┘ └──────┘ └────────┘
```

---

## Conclusion

All data models support TDD methodology with clear validation rules. Pydantic implementations (next section) provide type-safe Python classes with automatic validation, JSON serialization, and OpenAPI schema generation.

**Next**: See `/contracts/data-models/` for concrete Pydantic model implementations.
